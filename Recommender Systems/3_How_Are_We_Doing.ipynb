{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How Are We Doing?\n",
    "\n",
    "In the last notebook, you created a working version of SVD for situations even when there are tons of missing values.  This is awesome!  The question now is how well does this solution work?\n",
    "\n",
    "In this notebook, we are going to simulate exactly what we would do in the real world to tune our recommender.  \n",
    "\n",
    "Run the cell below to read in the data and get started."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Read in the datasets\n",
    "movies = pd.read_csv('movies_clean.csv')\n",
    "reviews = pd.read_csv('reviews_clean.csv')\n",
    "\n",
    "del movies['Unnamed: 0']\n",
    "del reviews['Unnamed: 0']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Using the **reviews** dataframe, perform the following tasks to create a training and validation set of data we can use to test the performance of your SVD algorithm using **off-line** validation techniques.\n",
    "\n",
    " * Order the reviews dataframe from earliest to most recent \n",
    " * Pull the first 10000 reviews from  the dataset\n",
    " * Make the first 8000/10000 reviews the training data \n",
    " * Make the last 2000/10000 the test data\n",
    " * Return the training and test datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_train_test(reviews, order_by, training_size, testing_size):\n",
    "    '''    \n",
    "    INPUT:\n",
    "    reviews - (pandas df) dataframe to split into train and test\n",
    "    order_by - (string) column name to sort by\n",
    "    training_size - (int) number of rows in training set\n",
    "    testing_size - (int) number of columns in the test set\n",
    "    \n",
    "    OUTPUT:\n",
    "    training_df -  (pandas df) dataframe of the training set\n",
    "    validation_df - (pandas df) dataframe of the test set\n",
    "    '''\n",
    "    # Implement your code here\n",
    "    reviews_new = reviews.sort_values(order_by)\n",
    "    training_df = reviews_new.head(training_size)\n",
    "    validation_df = reviews_new.iloc[training_size: training_size + testing_size]\n",
    "    \n",
    "    return training_df, validation_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use our function to create training and test datasets\n",
    "train_df, val_df = create_train_test(reviews, 'date', 8000, 2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nice job!  Looks like you have written a function that provides training and validation dataframes for you to use in the next steps.\n"
     ]
    }
   ],
   "source": [
    "# Make sure the dataframes we are using are the right shape\n",
    "assert train_df.shape[0] == 8000, \"The number of rows doesn't look right in the training dataset.\"\n",
    "assert val_df.shape[0] == 2000, \"The number of rows doesn't look right in the validation dataset\"\n",
    "assert str(train_df.tail(1)['date']).split()[1] == '2013-07-19', \"The last date in the training dataset doesn't look like what we expected.\"\n",
    "assert str(val_df.tail(1)['date']).split()[1] == '2013-08-18', \"The last date in the validation dataset doesn't look like what we expected.\"\n",
    "print(\"Nice job!  Looks like you have written a function that provides training and validation dataframes for you to use in the next steps.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the real world, we might have all of the data up to this final date in the training data.  Then we want to see how well we are doing for each of the new ratings, which show up in the test data.\n",
    "\n",
    "Below is a working example of the function created in the previous example you can use (or you can replace with your own).\n",
    "\n",
    "`2.`  Fit the function to the training data with the following hyperparameters: 15 latent features, a learning rate of 0.005, and 300 iterations. This will take some time to run, so you may choose fewer latent features, a higher learning rate, or fewer iteratios if you want to speed up the process.  \n",
    "\n",
    "**Note:** Again, this might be a good time to take a phone call, go for a walk, or just take a little break."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def FunkSVD(ratings_mat, latent_features=12, learning_rate=0.0001, iters=100):\n",
    "    '''\n",
    "    This function performs matrix factorization using a basic form of FunkSVD with no regularization\n",
    "    \n",
    "    INPUT:\n",
    "    ratings_mat - (numpy array) a matrix with users as rows, movies as columns, and ratings as values\n",
    "    latent_features - (int) the number of latent features used\n",
    "    learning_rate - (float) the learning rate \n",
    "    iters - (int) the number of iterations\n",
    "    \n",
    "    OUTPUT:\n",
    "    user_mat - (numpy array) a user by latent feature matrix\n",
    "    movie_mat - (numpy array) a latent feature by movie matrix\n",
    "    '''\n",
    "    \n",
    "    # Set up useful values to be used through the rest of the function\n",
    "    n_users = ratings_mat.shape[0]\n",
    "    n_movies = ratings_mat.shape[1]\n",
    "    num_ratings = np.count_nonzero(~np.isnan(ratings_mat))\n",
    "    \n",
    "    # initialize the user and movie matrices with random values\n",
    "    user_mat = np.random.rand(n_users, latent_features)\n",
    "    movie_mat = np.random.rand(latent_features, n_movies)\n",
    "    \n",
    "    # initialize sse at 0 for first iteration\n",
    "    sse_accum = 0\n",
    "    \n",
    "    # keep track of iteration and MSE\n",
    "    print(\"Optimizaiton Statistics\")\n",
    "    print(\"Iterations | Mean Squared Error \")\n",
    "    \n",
    "    # for each iteration\n",
    "    for iteration in range(iters):\n",
    "\n",
    "        # update our sse\n",
    "        old_sse = sse_accum\n",
    "        sse_accum = 0\n",
    "        \n",
    "        # For each user-movie pair\n",
    "        for i in range(n_users):\n",
    "            for j in range(n_movies):\n",
    "                \n",
    "                # if the rating exists\n",
    "                if ratings_mat[i, j] > 0:\n",
    "                    \n",
    "                    # compute the error as the actual minus the dot product of the user and movie latent features\n",
    "                    diff = ratings_mat[i, j] - np.dot(user_mat[i, :], movie_mat[:, j])\n",
    "                    \n",
    "                    # Keep track of the sum of squared errors for the matrix\n",
    "                    sse_accum += diff**2\n",
    "                    \n",
    "                    # update the values in each matrix in the direction of the gradient\n",
    "                    for k in range(latent_features):\n",
    "                        user_mat[i, k] += learning_rate * (2*diff*movie_mat[k, j])\n",
    "                        movie_mat[k, j] += learning_rate * (2*diff*user_mat[i, k])\n",
    "\n",
    "        # print results\n",
    "        print(\"%d \\t\\t %f\" % (iteration+1, sse_accum / num_ratings))\n",
    "        \n",
    "    return user_mat, movie_mat "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimizaiton Statistics\n",
      "Iterations | Mean Squared Error \n",
      "1 \t\t 9.515384\n",
      "2 \t\t 5.133635\n",
      "3 \t\t 3.764486\n",
      "4 \t\t 2.954388\n",
      "5 \t\t 2.398726\n",
      "6 \t\t 1.988925\n",
      "7 \t\t 1.673029\n",
      "8 \t\t 1.422107\n",
      "9 \t\t 1.218478\n",
      "10 \t\t 1.050622\n",
      "11 \t\t 0.910653\n",
      "12 \t\t 0.792951\n",
      "13 \t\t 0.693371\n",
      "14 \t\t 0.608751\n",
      "15 \t\t 0.536597\n",
      "16 \t\t 0.474886\n",
      "17 \t\t 0.421944\n",
      "18 \t\t 0.376373\n",
      "19 \t\t 0.337003\n",
      "20 \t\t 0.302855\n",
      "21 \t\t 0.273116\n",
      "22 \t\t 0.247108\n",
      "23 \t\t 0.224271\n",
      "24 \t\t 0.204137\n",
      "25 \t\t 0.186319\n",
      "26 \t\t 0.170492\n",
      "27 \t\t 0.156386\n",
      "28 \t\t 0.143771\n",
      "29 \t\t 0.132455\n",
      "30 \t\t 0.122275\n",
      "31 \t\t 0.113091\n",
      "32 \t\t 0.104785\n",
      "33 \t\t 0.097254\n",
      "34 \t\t 0.090412\n",
      "35 \t\t 0.084182\n",
      "36 \t\t 0.078498\n",
      "37 \t\t 0.073303\n",
      "38 \t\t 0.068546\n",
      "39 \t\t 0.064184\n",
      "40 \t\t 0.060177\n",
      "41 \t\t 0.056490\n",
      "42 \t\t 0.053094\n",
      "43 \t\t 0.049960\n",
      "44 \t\t 0.047066\n",
      "45 \t\t 0.044388\n",
      "46 \t\t 0.041907\n",
      "47 \t\t 0.039607\n",
      "48 \t\t 0.037471\n",
      "49 \t\t 0.035486\n",
      "50 \t\t 0.033638\n",
      "51 \t\t 0.031916\n",
      "52 \t\t 0.030310\n",
      "53 \t\t 0.028810\n",
      "54 \t\t 0.027408\n",
      "55 \t\t 0.026096\n",
      "56 \t\t 0.024867\n",
      "57 \t\t 0.023714\n",
      "58 \t\t 0.022633\n",
      "59 \t\t 0.021616\n",
      "60 \t\t 0.020661\n",
      "61 \t\t 0.019762\n",
      "62 \t\t 0.018914\n",
      "63 \t\t 0.018115\n",
      "64 \t\t 0.017361\n",
      "65 \t\t 0.016649\n",
      "66 \t\t 0.015976\n",
      "67 \t\t 0.015339\n",
      "68 \t\t 0.014736\n",
      "69 \t\t 0.014165\n",
      "70 \t\t 0.013623\n",
      "71 \t\t 0.013109\n",
      "72 \t\t 0.012621\n",
      "73 \t\t 0.012156\n",
      "74 \t\t 0.011715\n",
      "75 \t\t 0.011295\n",
      "76 \t\t 0.010895\n",
      "77 \t\t 0.010514\n",
      "78 \t\t 0.010151\n",
      "79 \t\t 0.009804\n",
      "80 \t\t 0.009473\n",
      "81 \t\t 0.009157\n",
      "82 \t\t 0.008855\n",
      "83 \t\t 0.008565\n",
      "84 \t\t 0.008289\n",
      "85 \t\t 0.008024\n",
      "86 \t\t 0.007770\n",
      "87 \t\t 0.007527\n",
      "88 \t\t 0.007294\n",
      "89 \t\t 0.007071\n",
      "90 \t\t 0.006856\n",
      "91 \t\t 0.006650\n",
      "92 \t\t 0.006452\n",
      "93 \t\t 0.006262\n",
      "94 \t\t 0.006079\n",
      "95 \t\t 0.005903\n",
      "96 \t\t 0.005733\n",
      "97 \t\t 0.005570\n",
      "98 \t\t 0.005413\n",
      "99 \t\t 0.005262\n",
      "100 \t\t 0.005116\n",
      "101 \t\t 0.004976\n",
      "102 \t\t 0.004840\n",
      "103 \t\t 0.004709\n",
      "104 \t\t 0.004583\n",
      "105 \t\t 0.004461\n",
      "106 \t\t 0.004343\n",
      "107 \t\t 0.004229\n",
      "108 \t\t 0.004119\n",
      "109 \t\t 0.004013\n",
      "110 \t\t 0.003910\n",
      "111 \t\t 0.003811\n",
      "112 \t\t 0.003715\n",
      "113 \t\t 0.003621\n",
      "114 \t\t 0.003531\n",
      "115 \t\t 0.003444\n",
      "116 \t\t 0.003359\n",
      "117 \t\t 0.003277\n",
      "118 \t\t 0.003198\n",
      "119 \t\t 0.003121\n",
      "120 \t\t 0.003046\n",
      "121 \t\t 0.002973\n",
      "122 \t\t 0.002903\n",
      "123 \t\t 0.002835\n",
      "124 \t\t 0.002769\n",
      "125 \t\t 0.002704\n",
      "126 \t\t 0.002642\n",
      "127 \t\t 0.002581\n",
      "128 \t\t 0.002522\n",
      "129 \t\t 0.002465\n",
      "130 \t\t 0.002409\n",
      "131 \t\t 0.002355\n",
      "132 \t\t 0.002303\n",
      "133 \t\t 0.002251\n",
      "134 \t\t 0.002202\n",
      "135 \t\t 0.002153\n",
      "136 \t\t 0.002106\n",
      "137 \t\t 0.002060\n",
      "138 \t\t 0.002016\n",
      "139 \t\t 0.001972\n",
      "140 \t\t 0.001930\n",
      "141 \t\t 0.001889\n",
      "142 \t\t 0.001849\n",
      "143 \t\t 0.001810\n",
      "144 \t\t 0.001771\n",
      "145 \t\t 0.001734\n",
      "146 \t\t 0.001698\n",
      "147 \t\t 0.001663\n",
      "148 \t\t 0.001628\n",
      "149 \t\t 0.001595\n",
      "150 \t\t 0.001562\n",
      "151 \t\t 0.001530\n",
      "152 \t\t 0.001499\n",
      "153 \t\t 0.001469\n",
      "154 \t\t 0.001439\n",
      "155 \t\t 0.001410\n",
      "156 \t\t 0.001382\n",
      "157 \t\t 0.001355\n",
      "158 \t\t 0.001328\n",
      "159 \t\t 0.001302\n",
      "160 \t\t 0.001276\n",
      "161 \t\t 0.001251\n",
      "162 \t\t 0.001227\n",
      "163 \t\t 0.001203\n",
      "164 \t\t 0.001179\n",
      "165 \t\t 0.001157\n",
      "166 \t\t 0.001134\n",
      "167 \t\t 0.001113\n",
      "168 \t\t 0.001091\n",
      "169 \t\t 0.001071\n",
      "170 \t\t 0.001050\n",
      "171 \t\t 0.001030\n",
      "172 \t\t 0.001011\n",
      "173 \t\t 0.000992\n",
      "174 \t\t 0.000973\n",
      "175 \t\t 0.000955\n",
      "176 \t\t 0.000938\n",
      "177 \t\t 0.000920\n",
      "178 \t\t 0.000903\n",
      "179 \t\t 0.000886\n",
      "180 \t\t 0.000870\n",
      "181 \t\t 0.000854\n",
      "182 \t\t 0.000839\n",
      "183 \t\t 0.000823\n",
      "184 \t\t 0.000809\n",
      "185 \t\t 0.000794\n",
      "186 \t\t 0.000780\n",
      "187 \t\t 0.000766\n",
      "188 \t\t 0.000752\n",
      "189 \t\t 0.000738\n",
      "190 \t\t 0.000725\n",
      "191 \t\t 0.000712\n",
      "192 \t\t 0.000700\n",
      "193 \t\t 0.000687\n",
      "194 \t\t 0.000675\n",
      "195 \t\t 0.000663\n",
      "196 \t\t 0.000652\n",
      "197 \t\t 0.000640\n",
      "198 \t\t 0.000629\n",
      "199 \t\t 0.000618\n",
      "200 \t\t 0.000608\n",
      "201 \t\t 0.000597\n",
      "202 \t\t 0.000587\n",
      "203 \t\t 0.000577\n",
      "204 \t\t 0.000567\n",
      "205 \t\t 0.000557\n",
      "206 \t\t 0.000547\n",
      "207 \t\t 0.000538\n",
      "208 \t\t 0.000529\n",
      "209 \t\t 0.000520\n",
      "210 \t\t 0.000511\n",
      "211 \t\t 0.000503\n",
      "212 \t\t 0.000494\n",
      "213 \t\t 0.000486\n",
      "214 \t\t 0.000478\n",
      "215 \t\t 0.000470\n",
      "216 \t\t 0.000462\n",
      "217 \t\t 0.000454\n",
      "218 \t\t 0.000447\n",
      "219 \t\t 0.000439\n",
      "220 \t\t 0.000432\n",
      "221 \t\t 0.000425\n",
      "222 \t\t 0.000418\n",
      "223 \t\t 0.000411\n",
      "224 \t\t 0.000404\n",
      "225 \t\t 0.000398\n",
      "226 \t\t 0.000391\n",
      "227 \t\t 0.000385\n",
      "228 \t\t 0.000379\n",
      "229 \t\t 0.000372\n",
      "230 \t\t 0.000366\n",
      "231 \t\t 0.000360\n",
      "232 \t\t 0.000355\n",
      "233 \t\t 0.000349\n",
      "234 \t\t 0.000343\n",
      "235 \t\t 0.000338\n",
      "236 \t\t 0.000332\n",
      "237 \t\t 0.000327\n",
      "238 \t\t 0.000322\n",
      "239 \t\t 0.000317\n",
      "240 \t\t 0.000312\n",
      "241 \t\t 0.000307\n",
      "242 \t\t 0.000302\n",
      "243 \t\t 0.000297\n",
      "244 \t\t 0.000293\n",
      "245 \t\t 0.000288\n",
      "246 \t\t 0.000283\n",
      "247 \t\t 0.000279\n",
      "248 \t\t 0.000275\n",
      "249 \t\t 0.000270\n",
      "250 \t\t 0.000266\n",
      "251 \t\t 0.000262\n",
      "252 \t\t 0.000258\n",
      "253 \t\t 0.000254\n",
      "254 \t\t 0.000250\n",
      "255 \t\t 0.000246\n",
      "256 \t\t 0.000242\n",
      "257 \t\t 0.000239\n",
      "258 \t\t 0.000235\n",
      "259 \t\t 0.000231\n",
      "260 \t\t 0.000228\n",
      "261 \t\t 0.000224\n",
      "262 \t\t 0.000221\n",
      "263 \t\t 0.000218\n",
      "264 \t\t 0.000214\n",
      "265 \t\t 0.000211\n",
      "266 \t\t 0.000208\n",
      "267 \t\t 0.000205\n",
      "268 \t\t 0.000202\n",
      "269 \t\t 0.000199\n",
      "270 \t\t 0.000196\n",
      "271 \t\t 0.000193\n",
      "272 \t\t 0.000190\n",
      "273 \t\t 0.000187\n",
      "274 \t\t 0.000184\n",
      "275 \t\t 0.000181\n",
      "276 \t\t 0.000179\n",
      "277 \t\t 0.000176\n",
      "278 \t\t 0.000173\n",
      "279 \t\t 0.000171\n",
      "280 \t\t 0.000168\n",
      "281 \t\t 0.000166\n",
      "282 \t\t 0.000163\n",
      "283 \t\t 0.000161\n",
      "284 \t\t 0.000159\n",
      "285 \t\t 0.000156\n",
      "286 \t\t 0.000154\n",
      "287 \t\t 0.000152\n",
      "288 \t\t 0.000149\n",
      "289 \t\t 0.000147\n",
      "290 \t\t 0.000145\n",
      "291 \t\t 0.000143\n",
      "292 \t\t 0.000141\n",
      "293 \t\t 0.000139\n",
      "294 \t\t 0.000137\n",
      "295 \t\t 0.000135\n",
      "296 \t\t 0.000133\n",
      "297 \t\t 0.000131\n",
      "298 \t\t 0.000129\n",
      "299 \t\t 0.000127\n",
      "300 \t\t 0.000125\n"
     ]
    }
   ],
   "source": [
    "# Create user-by-item matrix - nothing to do here\n",
    "train_user_item = train_df[['user_id', 'movie_id', 'rating', 'timestamp']]\n",
    "train_data_df = train_user_item.groupby(['user_id', 'movie_id'])['rating'].max().unstack()\n",
    "train_data_np = np.array(train_data_df)\n",
    "\n",
    "# Fit FunkSVD with the specified hyper parameters to the training data\n",
    "user_mat, movie_mat = FunkSVD(train_data_np, \n",
    "                              latent_features=15, \n",
    "                              learning_rate=0.005, \n",
    "                              iters=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you have created the **user_mat** and **movie_mat**, we can use this to make predictions for how users would rate movies, by just computing the dot product of the row associated with a user and the column associated with the movie.\n",
    "\n",
    "`3.` Use the comments in the function below to complete the **predict_rating** function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_rating(user_matrix, movie_matrix, user_id, movie_id):\n",
    "    '''\n",
    "    INPUT:\n",
    "    user_matrix - user by latent factor matrix\n",
    "    movie_matrix - latent factor by movie matrix\n",
    "    user_id - the user_id from the reviews df\n",
    "    movie_id - the movie_id according the movies df\n",
    "    \n",
    "    OUTPUT:\n",
    "    pred - the predicted rating for user_id-movie_id according to FunkSVD\n",
    "    '''\n",
    "    # Create series of users and movies in the right order\n",
    "    user_ids_series = np.array(train_data_df.index)\n",
    "    movie_ids_series = np.array(train_data_df.columns)\n",
    "    \n",
    "    # User row and Movie Column\n",
    "    user_row = np.where(user_ids_series == user_id)[0][0]\n",
    "    movie_col = np.where(movie_ids_series == movie_id)[0][0]\n",
    "    \n",
    "    # Take dot product of that row and column in U and V to make prediction\n",
    "    pred = np.dot(user_matrix[user_row, :], movie_matrix[:, movie_col])\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9.1522917393251682"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test your function with the first user-movie in the user-movie matrix (notice this is a nan)\n",
    "pred_val = predict_rating(user_mat, movie_mat, 2625, 169547)\n",
    "pred_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is great that you now have a way to make predictions. However it might be nice to get a little phrase back about the user, movie, and rating.\n",
    "\n",
    "`4.` Use the comments in the function below to complete the **predict_rating** function.  \n",
    "\n",
    "**Note:** The movie name doesn't come back in a great format, so you can see in the solution I messed around with it a bit just to make it a little nicer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_prediction_summary(user_id, movie_id, prediction):\n",
    "    '''\n",
    "    INPUT:\n",
    "    user_id - the user_id from the reviews df\n",
    "    movie_id - the movie_id according the movies df\n",
    "    prediction - the predicted rating for user_id-movie_id\n",
    "    '''\n",
    "    movie_name = str(movies[movies['movie_id'] == movie_id]['movie']) [5:]\n",
    "    movie_name = movie_name.replace('\\nName: movie, dtype: object', '')\n",
    "    print(\"For user {} we predict a {} rating for the movie {}.\".format(user_id, round(prediction, 2), str(movie_name)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For user 2625 we predict a 9.15 rating for the movie     American Beauty (1999).\n"
     ]
    }
   ],
   "source": [
    "# Test your function the the results of the previous function\n",
    "print_prediction_summary(2625, 169547, pred_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the ability to make predictions, let's see how well our predictions do on the test ratings we already have.  This will give an indication of how well have captured the latent features, and our ability to use the latent features to make predictions in the future!\n",
    "\n",
    "`5.` For each of the user-movie rating in the **val_df** dataset, compare the actual rating given to the prediction you would make.  How do your predictions do?  Do you run into any problems?  If yes, what is the problem?  Use the document strings and comments below to assist as you work through these questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The actual rating for user 6410 on movie 1245492 is 8.\n",
      " While the predicted rating is 7.0.\n",
      "The actual rating for user 2894 on movie 1602613 is 3.\n",
      " While the predicted rating is 4.0.\n",
      "The actual rating for user 2625 on movie 169547 is 10.\n",
      " While the predicted rating is 9.0.\n",
      "The actual rating for user 5490 on movie 770828 is 6.\n",
      " While the predicted rating is 6.0.\n"
     ]
    }
   ],
   "source": [
    "def validation_comparison(val_df, num_preds):\n",
    "    '''\n",
    "    INPUT:\n",
    "    val_df - the validation dataset created in the third cell above\n",
    "    num_preds - (int) the number of rows (going in order) you would like to make predictions for\n",
    "    \n",
    "    OUTPUT:\n",
    "    Nothing returned - print a statement about the prediciton made for each row of val_df from row 0 to num_preds\n",
    "    '''\n",
    "    val_users = np.array(val_df['user_id'])\n",
    "    val_movies = np.array(val_df['movie_id'])\n",
    "    val_ratings = np.array(val_df['rating'])\n",
    "    \n",
    "    \n",
    "    for idx in range(num_preds):\n",
    "        try:\n",
    "            pred = predict_rating(user_mat, movie_mat, val_users[idx], val_movies[idx])\n",
    "            print(\"The actual rating for user {} on movie {} is {}.\\n While the predicted rating is {}.\"\n",
    "                  .format(val_users[idx], val_movies[idx], val_ratings[idx], round(pred))) \n",
    "        except IndexError:\n",
    "            continue\n",
    "\n",
    "        \n",
    "# Perform the predicted vs. actual for the first 10 rows.  How does it look?\n",
    "validation_comparison(val_df, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data_sci_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "vscode": {
   "interpreter": {
    "hash": "107006bfa038b673fc600fac7cf34051d397bfb14e7ca73078275878ce86dc80"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
