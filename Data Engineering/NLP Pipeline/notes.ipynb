{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  How NLP Pipelines Work\n",
    "The 3 stages of an NLP pipeline are: Text Processing > Feature Extraction > Modeling.\n",
    "\n",
    "1. Text Processing: Take raw input text, clean it, normalize it, and convert it into a form that is suitable for feature extraction.\n",
    "\n",
    "2. Feature Extraction: Extract and produce feature representations that are appropriate for the type of NLP task you are trying to accomplish and the type of model you are planning to use.\n",
    "3. Modeling: Design a statistical or machine learning model, fit its parameters to training data, use an optimization procedure, and then use it to make predictions about unseen data.\n",
    "\n",
    "This process isn't always linear and may require additional steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Why Do We Need to Process Text?\n",
    "\n",
    "1. Extracting plain text: Textual data can come from a wide variety of sources: the web, PDFs, word documents, speech recognition systems, book scans, etc. Your goal is to extract plain text that is free of any source specific markup or constructs that are not relevant to your task.\n",
    "2. Reducing complexity: Some features of our language like capitalization, punctuation, and common words such as a, of, and the, often help provide structure, but don't add much meaning. Sometimes it's best to remove them if that helps reduce the complexity of the procedures you want to apply later.\n",
    "\n",
    "## What Text Processing Will You Do in This Lesson?\n",
    "You'll prepare text data from different sources with the following text processing steps:\n",
    "\n",
    "1. Cleaning to remove irrelevant items, such as HTML tags\n",
    "2. Normalizing by converting to all lowercase and removing punctuation\n",
    "3. Splitting text into words or tokens\n",
    "4. Removing words that are too common, also known as stop words\n",
    "5. Identifying different parts of speech and named entities\n",
    "6. Converting words into their dictionary forms, using stemming and lemmatization\n",
    "\n",
    "After performing these steps, your text will capture the essence of what was being conveyed in a form that is easier to work with."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalization\n",
    "\n",
    "Plain text is still human language with all its variations and bells and whistles so in normalization, we will try to reduce some of that complexity.\n",
    "\n",
    "### Capitalization Removal\n",
    "In the English language, the starting letter of the first word in any sentence is usually capitalized. All caps are sometimes used for emphasis and for stylistic reasons. While this is convenient for a human reader from the standpoint of a machine learning algorithm, it does not make sense to differentiate between variations that mean the same thing:\n",
    "\n",
    "- Car\n",
    "- car\n",
    "- CAR\n",
    "Therefore, we usually convert every letter in our text to a common case, usually lowercase, so that each word is represented by a unique token.\n",
    "\n",
    "Here's some sample text from a movie review:\n",
    "\n",
    "> The first time you see The Second Renaissance it may look boring. Look at it at least twice and definetly watch part 2. It will change your view of the matrix. Are the human people the ones who started the war ? Is AI a bad thing ?\n",
    "\n",
    "If we have the review stored in a variable called text, converting it to lowercase is a simple call to the lower method in Python.\n",
    "\n",
    "```\n",
    "# Convert to lowercase\n",
    "text = text.lower()\n",
    "print(text)\n",
    "```\n",
    "\n",
    "> Output\n",
    ">\n",
    "> the first time you see the second renaissance it may look boring. look at it at least twice and definetly watch part 2. it will change your view of the matrix. are the human people the ones who started the war ? is ai a bad thing ?\n",
    "\n",
    "Note all the letters that were changed.\n",
    "\n",
    "### Punctation Removal\n",
    "\n",
    "Other languages may or may not have a case equivalent but similar principles may apply depending on your NLP task, you may want to remove special characters like periods, question marks, and exclamation points from the text and only keep letters of the alphabet and maybe numbers.\n",
    "\n",
    "This is useful when looking at text documents as a whole in applications like document classification and clustering where the low level details doesn't affect the application.\n",
    "\n",
    "To do this we can use a regular expression that matches everything that is not a lowercase A to Z, uppercase A is Z, or digits zero to nine, and replaces them with a space.\n",
    "\n",
    "```\n",
    "import re\n",
    "\n",
    "### Remove punctuation characters\n",
    "text = re.sub(r\"[^a-zA-Z0-9\", \" \", text) # Anything that isn't A through Z or 0 through 9 will be replaced by a space\n",
    "print(text)\n",
    "```\n",
    "\n",
    "> Output\n",
    "> \n",
    "> the you he first time you see the second renaissance it may look boring look at it at least twice and definetly watch part 2 it will change  your view of the matrix are the human people the ones who started the war is ai a bad thing\n",
    "\n",
    "This approach avoids having to specify all punctuation characters, but you can use other regular expressions as well.\n",
    "\n",
    "Lowercase conversion and punctuation removal are the two most common text normalization steps. If and when you apply these steps depends on your end goal and how you design your pipeline.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization\n",
    "\n",
    "Token is a fancy term for a symbol that holds some meaning and is not typically split up any further.\n",
    "\n",
    "In natural language processing, our tokens are usually individual words. This means that the process of tokenization is simply splitting a sentence into a sequence of words. The simplest way to do this is using the split method which returns a list of words.\n",
    "\n",
    "> Input\n",
    ">\n",
    "> the you he first time you see the second renaissance it may look boring look at it at least twice and definetly watch part 2 it will change your view of the matrix are the human people the ones who started the war is ai a bad thing\n",
    "\n",
    "```\n",
    "# Split text into tokens (words)\n",
    "words = text.split()\n",
    "print(words)\n",
    "```\n",
    "\n",
    "> Output\n",
    ">\n",
    "> ['the', 'you', 'he', 'first', 'time', 'you', 'see', 'the', 'second', 'renaissance', 'it', 'may', 'look', 'boring', 'look', 'at', 'it', 'at', 'least', 'twice', 'and', 'definetly', 'watch', 'part', '2', 'it', 'will', 'change', 'your', 'view', 'of', 'the', 'matrix', 'are', 'the', 'human', 'people', 'the', 'ones', 'who', 'started', 'the', 'war', 'is', 'ai', 'a', 'bad', 'thing]\n",
    "\n",
    "Notice that it splits on whitespace characters (spaces, tabs, new lines, etc.) and will automatically ignoring two or more whitespace characters in a sequence, so it doesn't return blank strings. This can be further adjusted using optional parameters.\n",
    "\n",
    "### Natural Language Toolkit (NLTK)\n",
    "\n",
    "So far, we've only been using Python's built-in functionality, but some of these operations are much easier to perform using a library like Natural Language Toolkit (NLTK).\n",
    "\n",
    "The most common approach for splitting up text in NLTK is to use the word tokenized function from nltk.tokenize.\n",
    "\n",
    "```\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "#Split text into words using NLTK\n",
    "words = word_tokenize(text)\n",
    "print(words)\n",
    "```\n",
    "\n",
    "This performs the same task as split but has a few more features than the split method.\n",
    "\n",
    "For example if we gave it\n",
    "\n",
    "> Dr. Smith graduated from the University of Washington. He later started an analytics firm called Lux, which catered to enterprise customers.\n",
    "\n",
    "it would return the following\n",
    "\n",
    "> ['Dr.', 'Smith', 'graduated', 'from', 'the', 'University', 'of', 'Washington', '.', 'He', 'later', 'started', 'an', 'analytics', 'firm', 'called', 'Lux', ',', 'which', 'catered', 'to', 'enterprise', 'customers', '.']\n",
    "\n",
    "You'll notice that the punctuations are treated differently based on their position. For example, 'Dr.' has been tokenized as one word rather than being tokenized into two seperate entities 'Dr' and '.'. NLTK is using some rules or patterns to decide what to do with each punctuation.\n",
    "\n",
    "### NLTK's Sentence Tokenization\n",
    "\n",
    "There are instances you may need to split a longer document into sentence, this is something that might be done for translations. You can achieve this with NLTK using sent tokenize.\n",
    "\n",
    "```\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "# Split text into sentences\n",
    "sentences = sent_tokenize(text)\n",
    "print(sentences) \n",
    "```\n",
    "\n",
    ">Output\n",
    ">\n",
    ">['Dr.Smith graduated from the University of Washington.', 'He later started an analytics firm called Lux, which catered to enterprise customers.']\n",
    "\n",
    "Now one could tokenize based on words if needed.\n",
    "\n",
    "NLTK provide several other tokenizers and here are some of them:\n",
    "\n",
    "regular expression based tokenizer that can remove punctuation and perform tokenization in a single step\n",
    "tweet tokenizer that is aware of twitter handles, hash tags, and emoticons\n",
    "Reference:\n",
    "\n",
    "[nltk.tokenize package](http://www.nltk.org/api/nltk.tokenize.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stop Word Removal\n",
    "\n",
    "**Stop Words** are words that don't add a lot meaning to a sentence or phrase (i.e, is, the, in, at, etc.) and are often very common words.\n",
    "\n",
    "We want to remove them to simplify procedures down the pipeline.\n",
    "\n",
    "For example you may have the statement:\n",
    "\n",
    "Dogs are the best\n",
    "\n",
    "Even with removing \"are\" and \"the\", the positive sentiment about dogs is still conveyed.\n",
    "\n",
    "A common package that has a pre-set list of stop words is NLTK.\n",
    "\n",
    "```\n",
    "# List stop words from NLTK\n",
    "from nltk.corpus import stopwords\n",
    "print(stopwords.words(\"english\"))\n",
    "```\n",
    "\n",
    "The NLTK can be used on a list of words.\n",
    "\n",
    "```\n",
    "words = ['the', 'first', 'time', 'you', 'see', 'the', 'second', 'renaissance', 'it', 'may', 'look', 'boring', 'look', 'at', 'it', 'at', 'least', 'twice', 'and', 'definetly', 'watch', 'part', '2', 'it', 'will', 'change', 'your', 'view', 'of', 'the', 'matrix', 'are', 'the', 'human people', 'the', 'ones', 'who', 'started', 'the', 'war', 'is', 'ai', 'a', 'bad', 'thing']\n",
    "\n",
    "# Remove stop words\n",
    "words = [w for w in words if w not in stopwords.words(\"english\")]\n",
    "print(words)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part-of-Speach Tagging\n",
    "**Note**: Part-of-speech tagging using a predefined grammar like this is a simple, but limited, solution. It can be very tedious and error-prone for a large corpus of text, since you have to account for all possible sentence structures and tags!\n",
    "\n",
    "There are other more advanced forms of POS tagging that can learn sentence structures and tags from given data, including Hidden Markov Models (HMMs) and Recurrent Neural Networks (RNNs).\n",
    "\n",
    "NLTK has the ability to label the parts of speach of the words given.\n",
    "\n",
    "```\n",
    "from nltk import pos_tag\n",
    "\n",
    "# Tag parts of speach (PoS)\n",
    "sentence = word_tokenize(\"I always lie down to tell a lie.\")\n",
    "pos_tag(sentence)\n",
    "```\n",
    "\n",
    "output\n",
    "```\n",
    "[('I', 'PRP'),\n",
    " ('always', 'RB'),\n",
    " ('lie', 'VBP'),\n",
    " ('down', 'RP'),\n",
    " ('to', 'TO'),\n",
    " ('tell', 'VB'),\n",
    " ('a', 'DT'),\n",
    " ('lie', 'NN'),\n",
    " ('.', '.')]\n",
    " ```\n",
    "Custom grammar to parse an ambiguous sentence and will return the possible ways the sentence could be read.\n",
    "\n",
    "```\n",
    "# Define a cusom grammar\n",
    "my_grammar = nltk.CFG.fromstring(\"\"\"\n",
    "S -> NP VP\n",
    "PP -> P NP\n",
    "NP -> Det N | Det N PP | 'I'\n",
    "VP -> V NP | VP PP\n",
    "Det -> 'an' | 'my'\n",
    "N -> 'elephant' | 'pajamas'\n",
    "V -> 'shot'\n",
    "P -> 'in'\n",
    "\"\"\")\n",
    "parser = nltk.ChartParser(my_grammar)\n",
    "\n",
    "# Parse a sentence\n",
    "sentence = word_tokenize(\"I shot an elephant in my pajamas\")\n",
    "for tree in parser.parse(sentence):\n",
    "  print(tree)\n",
    "```\n",
    "\n",
    "This can be even further visualized with the draw function on the tree.\n",
    "```\n",
    "# Visualize parse trees\n",
    "for tree in parser.parse(sentence):\n",
    "  tree.draw()\n",
    "```\n",
    "To learn more about NLTK PoS\n",
    "\n",
    "- NLTK Documentation on pos_tag in this link to [Chapter 5. Categorizing and Tagging Words](https://www.nltk.org/book/ch05.html)\n",
    "- Stack Overflow thread on the tokens for pos_tag in this link to [What are all possible pos tags of NLTK?](https://stackoverflow.com/questions/15388831/what-are-all-possible-pos-tags-of-nltk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Named Entity Recognition\n",
    "\n",
    "Named Entity are nouns or noun phrases that refer to specific object, person, or place.\n",
    "\n",
    "To label these we can use the ne_chunk function in NLTK.\n",
    "\n",
    "```\n",
    "from nltk import pos_tag, ne_chunk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Recognize named entities in a tagged sentence\n",
    "ne_chunk(pos_tag(word_tokenize(\"Antonio joined Udacity Inc. in California.\")))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemming \n",
    "\n",
    "Stemming is the process of reducing a word to its stem or root form. For example, branching, branched, and branches all stem from the word branch.\n",
    "\n",
    "This is a very quick and rough process so sometime the result isn't a complete word. For example, caching, cached, caches would result in a stem \"cach\", but that isn't a word. But as long as all related words to cache results in the same stem still captures the common idea in the resultant stem.\n",
    "\n",
    "There are a few options from NLTK but in this example we will look at Porter.\n",
    "\n",
    "```\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "#Reduce words to their stems\n",
    "stemmed = [PorterStemmer().stem(w) for w in words]\n",
    "print(stemmed)\n",
    "```\n",
    "\n",
    "## Lemmatization\n",
    "\n",
    "Lemmatization is the process to map the words back to its root using a dictionary. For example, is, was, and were would all be lemmatized to \"be\".\n",
    "\n",
    "The default NLTK lemmatizer is wordnet.\n",
    "\n",
    "```\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "# Reduce words to their root form\n",
    "lemmed = [WordNetLemmatizer().lemmatize(w) for w in words]\n",
    "print(lemmed)\n",
    "```\n",
    "\n",
    "Lemmatizers need to know the part of speech and will default to nouns but we can add parameters to change which part of speech it will use.\n",
    "\n",
    "```\n",
    "# Lemmatize verbs by specifying pos\n",
    "lemmed = [WordNetLemmatizer().lemmatize(w, pos='v') for w in lemmed]\n",
    "print(lemmed)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's go over a summary of all the text processing we just covered.\n",
    "\n",
    "Text Processing\tExample\n",
    "\n",
    "| Given             | Jenna went back to University                 |\n",
    "|-------------------|-----------------------------------------------|\n",
    "| Normalized        | jenna went back to university                 |\n",
    "| Tokenized         | <\"jenna\", \"went\", \"back\", \"to\", \"University\"> |\n",
    "| Stop Word Removal | <\"jenna\", \"went\", \"university\">               |\n",
    "| Stem & lemmatized | <\"jenna\", \"go\", \"univers\">                    |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Extraction\n",
    "Each letter is represented using encodings like ASCII or Unicode so that each letter is represented by a number which is then stored or transmitted using binary (0s or 1s).\n",
    "\n",
    "Words, rather than letters themselves, hold meaning. But computers don't have a standard representation for words. Practically they are a sequence of binary, ASCII, or Unicode but the meaning and relationship between words is not easily captured with these methods.\n",
    "\n",
    "In comparison, an image's pixel value contains the relative intensity of light. For a color image we keep a value for each of the primary colors (red, green, and blue) which carry relavant information. This means that pixels with similar values are also visually similar. So pixel values can be used in a numerical model for images.\n",
    "\n",
    "### How can we do the same thing for image modeling with language?\n",
    "\n",
    "Depends on the model and goal of the model.\n",
    "\n",
    "For example, for a graph based model to extract insights you might create a web of nodes.\n",
    "\n",
    "But if you want a statistical model, you will need numerical representation.\n",
    "\n",
    "- If you are working at the document level (for spam detection or sentiment of the document) one would use bag-of-words or doc2vec.\n",
    "- If you are working at the individual words and phrases (for text generation or machine translation) one would use word2vec or glove.\n",
    "Practice will help over time to determine which is the best method for your use case.\n",
    "\n",
    "[WordNet visualization tool](http://mateogianolio.com/wordnet-visualization/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag of words\n",
    "Each document is turned into an unordered collection of words. For a plagiarism check for students in a class each submission or report could be considered a document. But if you are looking at sentiment in a tweet, each tweet would be considered a document.\n",
    "\n",
    "The first step is text processing and below is a table of the Given and result after text processing.\n",
    "\n",
    "\n",
    "| Given                       | Text Processed                |\n",
    "|-----------------------------|-------------------------------|\n",
    "| Little House on the Prairie | jenna went back to university |\n",
    "| Mary had a Little Lamb      | {\"mari\", \"littl\", \"lamb\"}     |\n",
    "| The Silence of the Lambs    | {\"silenc\", \"lamb\"}            |\n",
    "| Twinkle Twinkle Little Star | {\"twinkl\", \"littl\", \"star\"}   |\n",
    "\n",
    "The above table is a good start but the result doesn't represent that there was two \"Twinkle\"s from \"Twinkle Twinkle Little Star\". A better way to do this is with a Document-Term Matrix.\n",
    "\n",
    "This is usually done with a set of documents, known as a corpus (D).\n",
    "\n",
    "|                             | littl | hous | prairi | mari | lamb | silenc | twinkl | star |\n",
    "|-----------------------------|-------|------|--------|------|------|--------|--------|------|\n",
    "| Little House on the Prairie | 1     | 1    | 1      | 0    | 0    | 0      | 0      | 0    |\n",
    "| Mary had a Little Lamb      | 1     | 0    | 0      | 1    | 1    | 0      | 0      | 0    |\n",
    "| The Silence of the Lambs    | 0     | 0    | 0      | 0    | 1    | 1      | 0      | 0    |\n",
    "| Twinkle Twinkle Little Star | 1     | 0    | 0      | 0    | 0    | 0      | 2      | 1    |\n",
    "\n",
    "Each number in the vector is called a term frequency.\n",
    "\n",
    "### Comparing Documents\n",
    "\n",
    "Compare two documents based on how many words they have in common or how similar their terms frequencies are.\n",
    "\n",
    "|   |                             | littl | hous | prairi | mari | lamb | silenc | twinkl | star |\n",
    "|---|-----------------------------|-------|------|--------|------|------|--------|--------|------|\n",
    "| a | Little House on the Prairie | 1     | 1    | 1      | 0    | 0    | 0      | 0      | 0    |\n",
    "| b | Mary had a Little Lamb      | 1     | 0    | 0      | 1    | 1    | 0      | 0      | 0    |\n",
    "\n",
    "This is done mathematically by a dot product which is the sum of the products of corresponding elements. The larger the dot product will indicate that the two vectors are more similar.\n",
    "\n",
    "The dot product of A and B is the sum of A and B for each item. \n",
    "Dot Product\n",
    "$$\n",
    "  a\\cdot b = \\sum a_nb_n\n",
    "$$\n",
    "\n",
    "|                          |     |     |     |     |     |     |     |     |\n",
    "|--------------------------|-----|-----|-----|-----|-----|-----|-----|-----|\n",
    "| dot product of a and b = | 1*1 | 1*0 | 1*0 | 0*1 | 0*1 | 0*0 | 0*0 | 0*0 |\n",
    "| =                        | 1   | 0   | 0   | 0   | 0   | 0   | 0   | 0   |\n",
    "| =                        | 1   |     |     |     |     |     |     |     |\n",
    "\n",
    "The dot product only captures the overlap but doesn't take into account the values that don't overlap. Sometimes this can result in comparing two very different documents leads to a result as documents that are identical.\n",
    "\n",
    "The way to get around this is using cosine similarity. Which still uses the dot product as the numerator but will divide by the products of their magnitudes (Euclidean norms).\n",
    "\n",
    "$$\n",
    "  cos\\theta = \\frac{a\\cdot b}{|| a || \\times || b ||}\n",
    "$$\n",
    "Cosine of theta is the dot product of A and B divide by the product of normalized A and normalized B. \n",
    "Cosine Similarity\n",
    "\n",
    "This esentially makes each of the vectors an arrow pointing in a direction and then calculates the theta of the angle made by the arrow of A and B. We can look at this for comparison between a and b in the table below.\n",
    "\n",
    "| cos(theta) = dot(a, b) / \\|\\|a\\|\\| x \\|\\|b\\|\\| = | 1/3     |\n",
    "|--------------------------------------------------|---------|\n",
    "| dot product of a and b =                         | 1       |\n",
    "| \\|\\|a\\|\\|                                        | sqrt(3) |\n",
    "| \\|\\|b\\|\\|                                        | sqrt(3) |\n",
    "\n",
    "Identical documents will have a result of 1 and documents that don't share any similarities will have a result of -1. But documents that share approximately half will result in an orthogonal vector with a result of 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Document Frequency\n",
    "Bag of words treats each words as equally important. But based on our intiuition some words will occur more frequently in a corpus. For example, in financial documents, this corpus may have a high term frequency in terms like cost or price. To compensate for this we can count in how many documents each word occurs.\n",
    "\n",
    "|                             | littl| hous | prairi | mari | lamb | silenc | twinkl | star |\n",
    "|-----------------------------|------|--------|------|------|--------|--------|------|---|\n",
    "| Little House on the Prairie | 1    | 1      | 1    | 0    | 0      | 0      | 0    | 0 |\n",
    "| Mary had a Little Lamb      | 1    | 0      | 0    | 1    | 1      | 0      | 0    | 0 |\n",
    "| The Silence of the Lambs    | 0    | 0      | 0    | 0    | 1      | 1      | 0    | 0 |\n",
    "| Twinkle Twinkle Little Star | 1    | 0      | 0    | 0    | 0      | 0      | 2    | 1 |\n",
    "| Document Frequency          | 3    | 1      | 1    | 1    | 2      | 1      | 1    | 1 |\n",
    "\n",
    "Then divide the document Frequencies on all the values in the corpus. This now gives a proportional value of the term frequencies but is inversely proportional to how many documents that term appears in.\n",
    "\n",
    "|                             | littl | hous | prairi | mari | lamb | silenc | twinkl | star |\n",
    "|-----------------------------|-------|------|--------|------|------|--------|--------|------|\n",
    "| Little House on the Prairie | 1/3   | 1    | 1      | 0    | 0    | 0      | 0      | 0    |\n",
    "| Mary had a Little Lamb      | 1/3   | 0    | 0      | 1    | 1/2  | 0      | 0      | 0    |\n",
    "| The Silence of the Lambs    | 0     | 0    | 0      | 0    | 1/2  | 1      | 0      | 0    |\n",
    "| Twinkle Twinkle Little Star | 1/3   | 0    | 0      | 0    | 0    | 0      | 2      | 1    |\n",
    "| Document Frequency          | 3     | 1    | 1      | 1    | 2    | 1      | 1      | 1    |\n",
    "\n",
    "Values with a higher value (i.e., \"Mary\" and \"Silence\") are unique to a particular docment while smaller values mean they are frequently used throughout the corpus (i.e., \"Little\" or \"Lamb\"). This allows for better charaterization.\n",
    "\n",
    "## Term Frequency - Inverse Document Frequency (TF-IDF) Transform\n",
    "\n",
    "Includes two weights:\n",
    "\n",
    "- Term Frequency (tf)\n",
    "- Inverse Document Frequency (idf)\n",
    "\n",
    "### Term Frequency\n",
    "Is mathematically defined as the count of a term (t) in a document (d) divided by all the terms in the document.\n",
    "\n",
    "$$\n",
    "  tf(t,d) = \\frac{count(t,d)}{|d|}\n",
    "$$\n",
    "                                                      Term Frequency Mathematical Definition\n",
    "\n",
    "### Inverse Document Frequency\n",
    "Is the logarithm of the total number of documents in the coprpus (D) divided by the number of documents where the term (t) exists.\n",
    "\n",
    "$$\n",
    "    idf(t,D) = log (\\frac{|D|}{| \\{ d\\in D : t\\in d\\}|})\n",
    "$$\n",
    "\n",
    "                                                    Inverse Document Frequency Mathematical Definition\n",
    "\n",
    "### Resultant Equation of the TF-IDF\n",
    "These come together into the following mathematical formula.\n",
    "\n",
    "$$\n",
    "  tfidf(t,d,D) = tf(t,d) \\times idf(t,D)\n",
    "$$\n",
    "\n",
    "                                              Term Frequency - Inverse Document Frequency Mathematical Definition\n",
    "\n",
    "There are many variations that try to smooth or normalize the results or try to prevent edge cases and division by zero errors.\n",
    "\n",
    "But ultimately this is a good way to assign weight to words and indicate their relevance in a given document."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One-Hot Encodings\n",
    "One-Hot Encodings are used to have a numerical representation for each word in a document. It does they by treating each word as a class and assign it in a vector that only has one for where it is used and a zero for all other positons.\n",
    "\n",
    "|   | littl  | hous | prairi | mari | lamb | silenc | twinkl | star |\n",
    "|--------|------|--------|------|------|--------|--------|------|---|\n",
    "| hous   | 0    | 1      | 0    | 0    | 0      | 0      | 0    | 0 |\n",
    "| lamb   | 0    | 0      | 0    | 0    | 1      | 0      | 0    | 0 |\n",
    "| silenc | 0    | 0      | 0    | 0    | 0      | 1      | 0    | 0 |\n",
    "| twinkl | 0    | 0      | 0    | 0    | 0      | 0      | 1    | 0 |\n",
    "\n",
    "This may look similar to a bag of words but instead it is just a single word in each bag and build a vector for it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Embeddings\n",
    "\n",
    "As the number of words grows for a given dataset, One-hot encodings becomes less and less sustainable beacuse the size of the word representations grows with the number of words.\n",
    "\n",
    "This is where word embeddings comes in where it limits the word representation to a fixed-size vector. This means for each word we want to find the embedding in a vector space which exhibit desired properties.\n",
    "\n",
    "For example, words with similar meanings such as kid and child should be closer in comparison to words that have disparate meaning (i.e., rock).\n",
    "\n",
    "Another example, are words that are different in similar ways like man, king, woman, and queen. The distance between man and woman should be similar to the distance between king and queen.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling\n",
    "The final stage of the *NLP pipeline* is **modeling**, which includes designing a statistical or machine learning model, fitting its parameters to training data, using an optimization procedure, and then using it to make predictions about unseen data.\n",
    "\n",
    "The nice thing about working with numerical features is that it allows you to choose from all machine learning models or even a combination of them.\n",
    "\n",
    "Once you have a working model, you can deploy it as a web app, mobile app, or integrate it with other products and services. The possibilities are endless!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Embedding - Word2Vec\n",
    "\n",
    "Word2Vec is one of the most popular used word embeddings. As the name indicates it transforms words into vectors but let's look at how that transformation is done.\n",
    "\n",
    "The core idea is to predict a given word using neighboring words or the using a word to predict neighboring words. This indicates that the model is likely to have a strong grasp of contextual meaning of the words.\n",
    "\n",
    "There are 2 main cases:\n",
    "\n",
    "You are given a word and it predicts the neighboring words is called Continuous Skip-gram.\n",
    "You are given neigboring words is called continous bag of words (CBoW).\n",
    "Case 1: Skip-gram Model\n",
    "In the Skip-gram model, a word is chosen from a sentence. This word is converted into a one-hot encoded vector and fed into a neural network or probabilistic model. The model is designed to predict a few surrounding words, its context. We then would optimize the model's weights or parameters and repeat till it best predicts the surrounding words.\n",
    "\n",
    "Now, take an intermediate representation like a hidden layer in a neural network. The outputs of that layer for a given word become the corresponding word vector.\n",
    "\n",
    "Shows how the skip gram model would be given the word \"jumps\", turn it into a vector and then similar to a neural network would create word vectors and return \"brown\", \"fox\", \"over\", and \"the\" as neighboring words.\n",
    "\n",
    "![Skip Gram model](skip-gram-model.png)\n",
    "\n",
    "Case 2: Continuous Bag of Words (CBoW)\n",
    "Yields a very robust representation of words because the meaning of each word is distributed throughout the vector. The size of the word vector dependent on how you want to tune performance versus complexity. Unlike BoW, CBoW's vector size remains constant no matter how many words. Once trained on the traininig set (a large set of word vectors), you can just store them in a lookup table for future use.\n",
    "\n",
    "Now in a look up table it can be used in deep learning architectures. For example, it can be used as the input vector for recurrent neural nets. It is also possible to use RNNs to learn even better word embeddings. Some other optimizations are possible that further reduce the model and training complexity such as representing the output words using Hierarchical Softmax, computing loss using Sparse Cross Entropy, et cetera."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Global Vectors for Word Representation (GloVe)\n",
    "GloVe or global vectors for word representation is an approach of embedding that tries to directly optimize the vector representation of each word using co-occurrence statistics.\n",
    "\n",
    "First, the probability of a word j appears in the context of word i. For example, what is the probability that the word \"cup\" would be in the context (within 1-2 neighboring words) of the word \"coffee\"? The words \"cup\" and \"coffee\" are often related so we would intuit that it would be have a relatively high probability.\n",
    "\n",
    "Then, we count all such occurrences of i and j in our text collection, and then normalize a count to get a probability. Two random vectors are initialized for each word\n",
    "\n",
    "1. Word as a context\n",
    "2. Word as the target\n",
    "Now, for any pair of words, ij, we want the dot product of their word vectors.\n",
    "\n",
    "$$\n",
    "  P(j|i) = w_i \\times w_j\n",
    "$$\n",
    "\n",
    "                                                        Co-occurrence Probability Equation\n",
    "\n",
    "Using this as our goal and a suitable last function, we can iteratively optimize these word vectors. The result should be a set of vectors that capture the similarities and differences between individual words. If you look at it from another point of view, we are essentially factorizing the co-occurrence probability matrix into two smaller matrices. This is the basic idea behind GloVe. All that sounds good, but why co-occurrence probabilities?\n",
    "\n",
    "Consider this table and probabilities:\n",
    "\n",
    "|       | solid           | water           |\n",
    "|-------|-----------------|-----------------|\n",
    "| ice   | P(solid\\|ice)   | P(water\\|ice)   |\n",
    "| steam | P(solid\\|steam) | P(water\\|steam) |\n",
    "\n",
    "Using our intuition one would come across \"solid\" more often in the context of \"ice\" than \"steam\" and \"water\" could occur in either context with roughly equal probability. And that is what we see in the co-occurance probabilities.\n",
    "\n",
    "$$\n",
    "  \\frac{P(solid|ice)}{P(solid|steam)}>> 1               \n",
    "  \n",
    "$$\n",
    "$$\n",
    "                \n",
    "  \\frac{P(water|ice)}{P(water|steam)}\\approx 1\n",
    "$$\n",
    "\n",
    "                                              Probability comparison of ice, steam, solid, and water\n",
    "\n",
    "Given a large corpus, you'll find that the ratio of P solid given ice (P(solid|ice)) to P solid given steam (P(solid|steam)) is much greater than one, while the ratio of P water given ice (P(water|ice)) and P water given steam (P(water|steam))is close to one.\n",
    "\n",
    "Thus, we see that co-occurrence probabilities already exhibit some of the properties we want to capture. In fact, one refinement over using raw probability values is to optimize for the ratio of probabilities. The co-occurence probability matrix is huge and the co-occurrence probability values are typically very low, so it makes sense to work with the log of these values.\n",
    "\n",
    "I encourage you to read the paper that introduced GloVe to get a better understanding of this technique, called [GloVE: Global Vectors for Word Representations.](https://nlp.stanford.edu/pubs/glove.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding for Deep Learning\n",
    "\n",
    "Word embeddings are fast becoming the de facto choice for representing words, especially for use and deep neural networks. In the distributional hypothesis, states that words that occur in the same contexts tend to have similar meanings. For example, consider these sentences:\n",
    "\n",
    "***A**: Would you like to have a cup of <blank>?*\n",
    "\n",
    "***B**: I like my <blank> black.*\n",
    "\n",
    "***C**: I need my morning <blank> before I can do anything.*\n",
    "\n",
    "By now you probably have a word to fill in the <blank>. Let's look at some follow up questions:\n",
    "\n",
    "1. What would the blank be? \"Tea\" or \"Coffee\"\n",
    "2. What words in the sentence gave you the context clue for the word?\n",
    "- \"Cup\"\n",
    "- \"Black\"\n",
    "- \"Morning\"\n",
    "\n",
    "But it either \"Tea\" or \"Coffee\" could fill in the blanks and make sense. In these contexts, tea and coffee are actually similar. Therefore, when a large collection of sentences is used to learn in embedding, words with common context words tend to get pulled closer and closer together. Of course, there could also be contexts in which tea and coffee are dissimilar.\n",
    "\n",
    "For example:\n",
    "\n",
    "***A**: <blank> grounds are great for composting.*\n",
    "\n",
    "***B**: I prefer loose leaf <blank>.*\n",
    "\n",
    "A is clearly talking about \"coffee grounds\". While B is talking about \"loose leaf tea\".\n",
    "\n",
    "We can capture these similarities and differences in the same embedding by adding another dimension. Words can be close along one dimension. For example \"tea\" and \"coffee\" are both breverages but differ in other ways. A dimension could captures all the variability among beverages.\n",
    "\n",
    "In a human language, there are many more dimensions along which word meanings can vary. The more dimensions you can capture in your word vector, the more expressive that representation will be.\n",
    "\n",
    "### How many dimensions do you really need?\n",
    "For example, a typical neural network architecture designed for an NLP task like word prediction could have a few hundred dimension in a word embedding layer. This might seem large but remember using one-heart encodings is as large as the size of the vocabulary, sometimes in tens of thousands of words.\n",
    "\n",
    "You can also add learning embedding as part of the model training process and obtain a representation that captures the dimensions that are most relevant for your task. This often adds complexity so often we use a pre-trained embeddings (Word2Vec or GloVe) as a look-up unless your use case is very narrow like on for medical terminology. This will allow you to only train the layer specific to your task.\n",
    "\n",
    "Compare this with the network architecture for a computer vision task, say, image classification, the raw input here is also very high dimensional. For example, even 128 by 128 Image contains over 16 thousand pixels. We typically use convolutional layers to exploit the spatial relationships and image data and reduce this dimensionality. Early stages and visual processing are often transferable across tasks, so it is common to use some pre-trained layers from an existing network, like Alex nad or BTG 16 and only learn the later layers. Come to think of it, using an embedding look up for NLP is not on like using pre-treated layers for computer vision. Both are great examples of transfer learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# t-SNE\n",
    "Word embeddings need to have high dimensionality to capture sufficient variations in natural language, but this makes them hard to visualize.\n",
    "\n",
    "**t-Distributed Stochastic Neighbor Embedding (t-SNE)**, is a dimensionality reduction technique that can map high dimensional vectors to a lower dimensional space. It's kind of like Principle Component Analysis (PCA), but it tries to maintain relative distances between objects, so that similar ones stay closer together while dissimilar objects stay further apart.\n",
    "\n",
    "If we look at the larger vector space, we can discover meaningful groups of related words. Sometimes, that takes a while to realize why certain clusters are formed, but most of the groupings are very intuitive.\n",
    "\n",
    "T-SNE also works on other kinds of data, such as images. For example, pictures from the Caltech 101 dataset organized into clusters that roughly correspond to class labels\n",
    "\n",
    "- airplanes with blue sky\n",
    "- sailboats of different shapes and sizes\n",
    "- human faces\n",
    "\n",
    "This is a very useful tool for better understanding the representation that a network learns and for identifying any bugs or other issues."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.15 ('data_sci_env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "107006bfa038b673fc600fac7cf34051d397bfb14e7ca73078275878ce86dc80"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
