{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Practical Significance\n",
    "Even if an experiment result shows a statistically significant difference in an evaluation metric between control and experimental groups, that does not necessarily mean that the experiment was a success. If there are any costs associated with deploying a change, those costs might outweigh the benefits expected based on the experiment results. Practical significance refers to the level of effect that you need to observe in order for the experiment to be called a true success and implemented in truth. Not all experiments imply a practical significance boundary, but it's an important factor in the interpretation of outcomes where it is relevant.\n",
    "\n",
    "If you consider the confidence interval for an evaluation metric statistic against the null baseline and practical significance bound, there are a few cases that can come about.\n",
    "\n",
    "![stats1](stats1.jpg)\n",
    "![stats2](stats2.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Dummy Tests\n",
    "When it comes to designing an experiment, it might be useful to run a dummy test as a predecessor to or as part of that process. In a dummy test, you will implement the same steps that you would in an actual experiment to assign the experimental units into groups. However, the experimental manipulation won't actually be implemented, and the groups will be treated equivalently.\n",
    "\n",
    "There are multiple reasons to run a dummy test. First, a dummy test can expose if there are any errors in the randomization or assignment procedures. A short dummy test can be worth the investment if an invariant metric is found to have a statistically significant difference, or if some other systematic bias is identified because it can help avoid larger problems down the line. A second reason to run a dummy test is to collect data on metrics' behaviors. If historic data is not enough to predict the outcome of recorded metrics or allow for experiment duration to be computed, then a dummy test can be useful for getting baselines.\n",
    "\n",
    "Of course, performing a dummy test requires an investment of resources, the most important of which is time. If time is of the essence, then you may need to just go ahead with the experiment, keeping an eye on invariant metrics for any trouble. An alternative approach is to perform a hybrid test. In the A/B testing paradigm, this can take the form of an A/A/B test. That is, we split the data into three groups: two control and one experimental. A comparison between control groups can be used to learn about null-environment properties before making inferences on the effect of the experimental manipulation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Missing Data\n",
    "Three strategies for working with missing values include:\n",
    "\n",
    "- We can remove (or “drop”) the rows or columns holding the missing values.\n",
    "- We can impute the missing values.\n",
    "- We can build models that work around them, and only use the information provide\n",
    "\n",
    "Though dropping rows and/or columns holding missing values is quite easy to do using numpy and pandas, it is often not appropriate.\n",
    "\n",
    "Understanding why the data is missing is important before dropping these rows and columns. In this video you saw a number of situations in which dropping values was not a good idea. These included\n",
    "\n",
    "1. Dropping data values associated with the effort or time an individual put into a survey.\n",
    "2. Dropping data values associated with sensitive information.\n",
    "\n",
    "In either of these cases, the missing values hold information. A quick removal of the rows or columns associated with these missing values would remove missing data that could be used to better inform models.\n",
    "\n",
    "Instead of removing these values, we might keep track of the missing values using indicator values, or counts associated with how many questions an individual skipped.\n",
    "\n",
    "### When is it OK to remove data?\n",
    "A few instances in which dropping a row might be okay are:\n",
    "\n",
    "1. Dropping missing data associated with mechanical failures.\n",
    "2. The missing data is in a column that you are interested in predicting.\n",
    "Other cases when you should consider dropping data that are not associated with missing data:\n",
    "\n",
    "1. Dropping columns with no variability in the data.\n",
    "2. Dropping data associated with information that you know is not correct.\n",
    "In handling removing data, you should think more about why is this missing or why is this data incorrectly input to see if an alternative solution might be used than dropping the values.\n",
    "\n",
    "### Other considerations when removing data\n",
    "One common strategy for working with missing data is to understand the proportion of a column that is missing. If a large proportion of a column is missing data, this is a reason to consider dropping it.\n",
    "\n",
    "There are easy ways to use pandas to create dummy variables to track the missing values, so you can see if these missing values actually hold information (regardless of the proportion that is missing) before choosing to remove a full column.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyzing Multiple Metrics\n",
    "If you have multiple evaluation metrics to track in your experiment, you should be careful to specify the criteria for calling your experiment a success so that you don't make excessive errors in your judgment. To make things simple, this includes:\n",
    "\n",
    "- Assume independent measures\n",
    "- Success if either metric shows statistical significance\n",
    "- 5% Type I error rate\n",
    "\n",
    "Given the above assumptions, what is the probability that we falsely called the experiment is a success, assuming that there is no actual effect of our changes?\n",
    "\n",
    "![stats3](stats3.jpg)\n",
    "![stats4](stats4.jpg)\n",
    "![stats5](stats5.jpg)\n",
    "![stats6](stats6.jpg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Early Stopping\n",
    "As the workspace below shows, there are significant risks for peeking ahead and making an early decision if it is not planned for in the design. If you haven't accounted for the effects of peeking on your error rate, then it's best to resist the temptation to look at the results early, and only perform a final analysis at the end of the experiment. This is another reason why it's important to design an experiment ahead of any data collection.\n",
    "\n",
    "Note that there are ways of putting together a design to allow for making an early decision on an experiment. In the workspace, we showed how to treat the a problem like a multiple comparisons problem, adjusting the individual test-wise error rate to preserve an overall error rate. For continuous tracking, this page describes a rule of thumb for rate-based metrics, tracking the number of successes in each group and stopping the experiment once the counts' sum or difference exceeds some threshold. More generally, tests like the [sequential probability ratio test](https://en.wikipedia.org/wiki/Sequential_probability_ratio_test) can be developed to make an early stopping decision while an experiment is running, if it looks statistically unlikely for a metric to move past or fall back against the statistical significance bound."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GLOSSARY\n",
    "\n",
    "| Key Term               | Definition                                                                                                                                                                                        |\n",
    "|------------------------|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n",
    "| Bootstrapping          | Estimate sampling distributions by using actually collected data to generate new samples that could have been hypothetically collected.                                                           |\n",
    "| Non-parametric tests   | Tests that don't rely on many assumptions of the underlying population, and so can be used in a wider range of circumstances compared to standard tests.                                          |\n",
    "| Permutation Tests      | A resampling-type test used to compare the values on an outcome variable between two or more groups.                                                                                              |\n",
    "| Practical significance | Refers to the level of effect that you need to observe in order for the experiment to be called a true success and implemented in truth.                                                          |\n",
    "| Rank-Sum test          | This test is performed only on the data present. Also known as the Mann-Whitney U test, is not a test of any particular statistic like the mean or median. Instead, it's a test of distributions. |\n",
    "| Sign test              | This test only requires that there be paired values between two groups to compare, and tests whether one group's values tend to be higher than the other's.                                       |"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
